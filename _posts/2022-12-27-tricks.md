---
title: 'Engineering Tricks'
date: 2022-12-27
permalink: /posts/2022/12/engineering-tricks/
tags:
  - advice
---
Compilation of tricks I find useful.

## Software Engineering
- **[Set up passwordless ssh](https://linuxize.com/post/how-to-setup-passwordless-ssh-login/).** If you still have issues, it's likely file permissions. Set permissions according to [this](https://superuser.com/questions/215504/permissions-on-private-key-in-ssh-folder).
- **[Project Setup Boilerplate](https://goodresearch.dev/setup.html)**. Finally doing away with relative imports!
- **[Gitignore Boilerplate](https://github.com/github/gitignore/blob/main/Python.gitignore)**. Initialise gitignore from here.
- **Commands to create venv**

```
python3.9 -m venv name-of-venv-folder
# python3.9 -m venv name-of-venv-folder --system-site-packages  
source name-of-venv-folder/bin/activate
```

- **Find kernel for jupyter notebook.** (general jupyter notebook, not jupyter notebook + VScode) 

```
# activate env
source env/bin/activate

# ensures jupyter and python is in same environment. See https://stackoverflow.com/questions/48193822/import-of-package-works-in-ipython-shell-but-not-in-jupyter-notebook
pip install notebook --ignore-installed  

# these needs to be same
which python3
which jupyter

# use jupyter notebook in venv. See
ipython kernel install --user --name=venv

# to see env, simply refresh
```

For Jupyter notebook + VScode, ensure that you've installed the following extensions: jupyter, python.

## HuggingFace
- **Sharing datasets**. Guides on how to share my own HF datasets.
  - [Sharing](https://huggingface.co/docs/datasets/share)
  - [Create a dataset loading script](https://huggingface.co/docs/datasets/dataset_script#create-a-dataset-loading-script)
  - [Create a dataset card](https://huggingface.co/docs/datasets/dataset_card) 

## LLMs Training
### Datasets
- Use [HF datasets](https://huggingface.co/docs/datasets/index), which comes with multiprocessing and caching. Note that for distributed setting, we'll need to set up barriers. See more [here](https://huggingface.co/docs/datasets/process#multiprocessing)

### Training
- **To tackle self-attention memory cost of $O(N^2)$, use xformers which reduces the memory required for self-attention complexity to $O(logn)$**. See this PR of [vicuna](https://github.com/lm-sys/FastChat/pull/1255)
- **If system CPU utilization is too high, set number of threads to 1 via `torch.set_num_threads(1)`**. See more [here](https://discuss.pytorch.org/t/cpu-usage-far-too-high-and-training-inefficient/57228).
- **[Importance of CUDA versions](https://stackoverflow.com/questions/9727688/how-to-get-the-cuda-version)**. Running `nvidia-smi` will show the **highest CUDA version the installed driver supports** on the top right corner of the comand's output. `nvcc-version` shows the **cuda version currently installed**. Make sure they tally! Additionally, make sure your packages satisfy the cuda version (e.g. don't install torch 2.0 with cuda 11.6)
- **If LLM is pre-trained in bf16, finetune it with bf16 (not fp16).** See more [h[here](https://huggingface.co/docs/transformers/v4.13.0/en/performance#bf16)
- `estimate_zero3_model_states_mem_needs_all_cold` to find estimate memory needed for full finetuning. See this [link](https://huggingface.co/docs/transformers/main_classes/deepspeed#memory-requirements) 
  - Should set `total_params` to params of model; directly passing in model tends to undercalculate # of params.
- **Packing with attention segment masking.** See this twitter thread [here](https://twitter.com/agihippo/status/1645798187339505666). Credits to the twitters in thread.
- **4 bit optimiser PEFT training**. See more [here](https://huggingface.co/blog/4bit-transformers-bitsandbytes).

## LLM Inference
- **8 bit inferece**. 

```
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME_OR_PATH,
                                             device_map="auto",
                                             load_in_8bit=True,)
model.eval();
```

> To support 8 bit inference for your own models, follow this [discussion](https://github.com/huggingface/transformers/issues/22488). 

- **Generate text excluding prompt.**

```
def remove_input_ids_from_output_ids(input_ids, output_ids, tokenizer):
    """ Remove input_ids from output_ids. Applicable only for causalLM models, which output input_ids in outputs."""
    input_ids_lens = torch.tensor([
    tokenized.ne(tokenizer.pad_token_id).sum().item() for tokenized in input_ids])
    padding_lens = torch.tensor([(tokenized == tokenizer.pad_token_id).sum().item() for tokenized in input_ids])
    total_lens = input_ids_lens + padding_lens
    outputs = [op[total_lens[i]:] for i, op in enumerate(output_ids)]
    return outputs
```

## Docker/Singularity
- **To run docker for multinode training, set up network bridge between containers.** See [here](https://docs.sylabs.io/guides/3.1/user-guide/networking.html).
