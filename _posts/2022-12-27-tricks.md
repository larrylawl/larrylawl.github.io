---
title: 'Engineering Tricks'
date: 2022-12-27
permalink: /posts/2022/12/engineering-tricks/
tags:
  - advice
---
Compilation of tricks I find useful.

### Software Engineering
- **[Project Setup Boilerplate](https://goodresearch.dev/setup.html)**. Finally doing away with relative imports!
- **[Gitignore Boilerplate](https://github.com/github/gitignore/blob/main/Python.gitignore)**. Initialise gitignore from here.
- **Commands to create venv**

```
python3.9 -m venv name-of-venv-folder
# python3.9 -m venv name-of-venv-folder --system-site-packages  
source name-of-venv-folder/bin/activate
```

- **Find kernel for jupyter notebook.** (general jupyter notebook, not jupyter notebook + VScode) 

```
# activate env
source env/bin/activate

# ensures jupyter and python is in same environment. See https://stackoverflow.com/questions/48193822/import-of-package-works-in-ipython-shell-but-not-in-jupyter-notebook
pip install notebook --ignore-installed  

# these needs to be same
which python3
which jupyter

# use jupyter notebook in venv. See
ipython kernel install --user --name=venv

# to see env, simply refresh
```

For Jupyter notebook + VScode, ensure that you've installed the following extensions: jupyter, python.

### HuggingFace
- **Sharing datasets**. Guides on how to share my own HF datasets.
  - [Sharing](https://huggingface.co/docs/datasets/share)
  - [Create a dataset loading script](https://huggingface.co/docs/datasets/dataset_script#create-a-dataset-loading-script)
  - [Create a dataset card](https://huggingface.co/docs/datasets/dataset_card) 

### LLMs Training
- **If LLM is pre-trained in bf16, finetune it with bf16 (not fp16).** See more [here](https://huggingface.co/docs/transformers/v4.13.0/en/performance#bf16)
- `estimate_zero3_model_states_mem_needs_all_cold` to find estimate memory needed for full finetuning. See this [link](https://huggingface.co/docs/transformers/main_classes/deepspeed#memory-requirements) 
  - Should set `total_params` to params of model; directly passing in model tends to undercalculate # of params.

```
# for gpt-neox-20b
from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live, estimate_zero3_model_states_mem_needs_all_cold
estimate_zero3_model_states_mem_needs_all_cold(total_params=2e10,
                                               largest_layer_params=309e6,
                                               num_gpus_per_node=2, 
                                               num_nodes=1)
```

### LLMs Inference
- **[Generate text excluding prompt.](https://github.com/huggingface/transformers/issues/17117)**

```
gen_text = tokenizer.batch_decode(gen_tokens[input_ids.shape[0]:])[0]
```
