---
title: 'Engineering Tricks'
date: 2022-12-27
permalink: /posts/2022/12/engineering-tricks/
tags:
  - advice
---
Compilation of tricks I find useful.

### Software Engineering
- **[Project Setup Boilerplate](https://goodresearch.dev/setup.html)**. Finally doing away with relative imports!
- **[Gitignore Boilerplate](https://github.com/github/gitignore/blob/main/Python.gitignore)**. Initialise gitignore from here.
- **Commands to create venv**

```
python3.9 -m venv name-of-venv-folder
# python3.9 -m venv name-of-venv-folder --system-site-packages  
source name-of-venv-folder/bin/activate
```

- **Find kernel for jupyter notebook.** (general jupyter notebook, not jupyter notebook + VScode) 

```
# activate env
source env/bin/activate

# ensures jupyter and python is in same environment. See https://stackoverflow.com/questions/48193822/import-of-package-works-in-ipython-shell-but-not-in-jupyter-notebook
pip install notebook --ignore-installed  

# these needs to be same
which python3
which jupyter

# use jupyter notebook in venv. See
ipython kernel install --user --name=venv

# to see env, simply refresh
```

For Jupyter notebook + VScode, ensure that you've installed the following extensions: jupyter, python.

### HuggingFace
- **Sharing datasets**. Guides on how to share my own HF datasets.
  - [Sharing](https://huggingface.co/docs/datasets/share)
  - [Create a dataset loading script](https://huggingface.co/docs/datasets/dataset_script#create-a-dataset-loading-script)
  - [Create a dataset card](https://huggingface.co/docs/datasets/dataset_card) 

### LLMs Training
- **If LLM is pre-trained in bf16, finetune it with bf16 (not fp16).** See more [here](https://huggingface.co/docs/transformers/v4.13.0/en/performance#bf16)
- `estimate_zero3_model_states_mem_needs_all_cold` to find estimate memory needed for full finetuning. See this [link](https://huggingface.co/docs/transformers/main_classes/deepspeed#memory-requirements) 
  - Should set `total_params` to params of model; directly passing in model tends to undercalculate # of params.
- **Llama + Flash Attention**. See more [here](https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train_mem.py). Credits to Vicuna repo.
- **Packing with attention segment masking.** See this twitter thread [here](https://twitter.com/agihippo/status/1645798187339505666). Credits to the twitters in thread.
- **8 bit optimiser training**. Simply pass `adamw_bnb_8bit` in trainer.

### LLM Inference
- **8 bit inferece**. 

```
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME_OR_PATH,
                                             device_map="auto",
                                             load_in_8bit=True,)
model.eval();
```

> To support 8 bit inference for your own models, follow this [discussion](https://github.com/huggingface/transformers/issues/22488). 

- **Generate text excluding prompt.**

```
def remove_input_ids_from_output_ids(input_ids, output_ids, tokenizer):
    """ Remove input_ids from output_ids. Applicable only for causalLM models, which output input_ids in outputs."""
    input_ids_lens = torch.tensor([
    tokenized.ne(tokenizer.pad_token_id).sum().item() for tokenized in input_ids])
    padding_lens = torch.tensor([(tokenized == tokenizer.pad_token_id).sum().item() for tokenized in input_ids])
    total_lens = input_ids_lens + padding_lens
    outputs = [op[total_lens[i]:] for i, op in enumerate(output_ids)]
    return outputs
```
